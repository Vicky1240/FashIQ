{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_retrieval(ground_truth, predicted, similarity_scores, k=5):\n",
    "    \"\"\"\n",
    "    Evaluates image retrieval performance.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (dict): {query: [relevant_image_ids]}\n",
    "        predicted (dict): {query: [retrieved_image_ids]}\n",
    "        similarity_scores (dict): {query: [cosine_sim_scores aligned with predicted]}\n",
    "        k (int): top-K value to evaluate\n",
    "\n",
    "    Returns:\n",
    "        dict: containing precision@k, recall@k, top-k accuracy\n",
    "    \"\"\"\n",
    "    precision_list, recall_list, top_k_correct = [], [], 0\n",
    "    all_cosine_scores = []\n",
    "\n",
    "    for query in ground_truth:\n",
    "        gt_ids = set(ground_truth[query])\n",
    "        pred_ids = predicted.get(query, [])[:k]\n",
    "        sims = similarity_scores.get(query, [])[:k]\n",
    "\n",
    "        all_cosine_scores.extend(sims)\n",
    "\n",
    "        relevant_retrieved = gt_ids.intersection(pred_ids)\n",
    "        precision = len(relevant_retrieved) / k\n",
    "        recall = len(relevant_retrieved) / len(gt_ids) if gt_ids else 0\n",
    "        top_k_hit = any(img_id in gt_ids for img_id in pred_ids)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        top_k_correct += 1 if top_k_hit else 0\n",
    "\n",
    "    results = {\n",
    "        \"Precision@K\": round(np.mean(precision_list), 4),\n",
    "        \"Recall@K\": round(np.mean(recall_list), 4),\n",
    "        \"Top-K Accuracy\": round(top_k_correct / len(ground_truth), 4),\n",
    "    }\n",
    "\n",
    "    # Plot cosine similarity distribution\n",
    "    plt.hist(all_cosine_scores, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(\"Cosine Similarity Distribution\")\n",
    "    plt.xlabel(\"Cosine Similarity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "import streamlit as st\n",
    "import os, json, requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from snowflake_list import annotations_list\n",
    "from typing import Optional\n",
    "import boto3\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/stenv/lib/python3.9/site-packages/snowflake/connector/options.py:108: UserWarning: You have an incompatible version of 'pyarrow' installed (19.0.1), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "from pages.Stylist import interact_with_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_CLIP    = \"http://127.0.0.1:8000/image-search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries=[\n",
    "    'outfit for a party',\n",
    "    'Give me an outfit for a day outing. I would like it to be casual and calm',\n",
    "    'What would you suggest if I am going on a date?',\n",
    "    'What dress would you suggest me if I am going on a road trip? I want it to be a T-shirt.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=dict()\n",
    "for query in queries:\n",
    "    recs = interact_with_gpt(query)\n",
    "    top = recs[\"Top\"]\n",
    "    gender = \"Men\"\n",
    "    for part in ([\"Top\"]):\n",
    "        item = recs[part]\n",
    "        keyword = \"+\".join([item[\"color\"],item[\"clothing type\"],item[\"pattern\"],gender])\n",
    "        # 1) fetch similar images via your FastAPI\n",
    "        img_resp = requests.post(URL_CLIP, json={\"query\": keyword})\n",
    "        predicted[query]=img_resp.json()[\"image_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    \"outfit for a party\": [\"22.jpg\", \"24.jpg\", \"5.jpg\"],\n",
    "    \"Give me an outfit for a day outing. I would like it to be casual and calm\": ['14.jpg','1.jpg','33.jpg'],\n",
    "    \"What would you suggest if I am going on a date?\": ['26.jpg', '18.jpg','27.jpg'],\n",
    "    \"What dress would you suggest me if I am going on a road trip? I want it to be a T-shirt.\": ['15.jpg','13.jpg','12.jpg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outfit for a party': ['28.jpg', '18.jpg'],\n",
       " 'Give me an outfit for a day outing. I would like it to be casual and calm': ['31884252_fpx.png',\n",
       "  '17.jpg'],\n",
       " 'What would you suggest if I am going on a date?': ['1.jpg', '2.jpg'],\n",
       " 'What dress would you suggest me if I am going on a road trip? I want it to be a T-shirt.': ['15.jpg',\n",
       "  '13.jpg']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_retrieval(ground_truth, predicted, k=5):\n",
    "    \"\"\"\n",
    "    Evaluates image retrieval performance.\n",
    "\n",
    "    Args:\n",
    "        ground_truth (dict): {query: [relevant_image_ids]}\n",
    "        predicted (dict): {query: [retrieved_image_ids]}\n",
    "        similarity_scores (dict): {query: [cosine_sim_scores aligned with predicted]}\n",
    "        k (int): top-K value to evaluate\n",
    "\n",
    "    Returns:\n",
    "        dict: containing precision@k, recall@k, top-k accuracy\n",
    "    \"\"\"\n",
    "    precision_list, recall_list, top_k_correct = [], [], 0\n",
    "\n",
    "    for query in ground_truth:\n",
    "        gt_ids = set(ground_truth[query])\n",
    "        pred_ids = predicted.get(query, [])[:k]\n",
    "\n",
    "        relevant_retrieved = gt_ids.intersection(pred_ids)\n",
    "        precision = len(relevant_retrieved) / k\n",
    "        recall = len(relevant_retrieved) / len(gt_ids) if gt_ids else 0\n",
    "        top_k_hit = any(img_id in gt_ids for img_id in pred_ids)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        top_k_correct += 1 if top_k_hit else 0\n",
    "\n",
    "    results = {\n",
    "        \"Precision@K\": round(np.mean(precision_list), 4),\n",
    "        \"Recall@K\": round(np.mean(recall_list), 4),\n",
    "        \"Top-K Accuracy\": round(top_k_correct / len(ground_truth), 4),\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision@K': np.float64(0.25), 'Recall@K': np.float64(0.0833), 'Top-K Accuracy': 0.25}\n",
      "{'Precision@K': np.float64(0.1667), 'Recall@K': np.float64(0.1667), 'Top-K Accuracy': 0.25}\n",
      "{'Precision@K': np.float64(0.1), 'Recall@K': np.float64(0.1667), 'Top-K Accuracy': 0.25}\n"
     ]
    }
   ],
   "source": [
    "for i in [1,3,5]:\n",
    "    print(evaluate_retrieval(ground_truth, predicted, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ---- 1. GPT-Based Evaluation: Text Recommendations ----\n",
    "\n",
    "def compute_bleu_scores(references, candidates):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu2 = np.mean([\n",
    "        sentence_bleu([ref.split()], cand.split(), weights=(0.5, 0.5), smoothing_function=smoothie)\n",
    "        for ref, cand in zip(references, candidates)\n",
    "    ])\n",
    "    bleu4 = np.mean([\n",
    "        sentence_bleu([ref.split()], cand.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "        for ref, cand in zip(references, candidates)\n",
    "    ])\n",
    "    return bleu2, bleu4\n",
    "\n",
    "def compute_rouge_l(references, candidates):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, cand)['rougeL'].fmeasure for ref, cand in zip(references, candidates)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Sample input (replace with actual model predictions and human references)\n",
    "references = [\"a floral summer dress perfect for beach outings\"]\n",
    "candidates = [\"a summer floral dress good for beach outings\"]\n",
    "\n",
    "bleu2, bleu4 = compute_bleu_scores(references, candidates)\n",
    "rouge_l = compute_rouge_l(references, candidates)\n",
    "\n",
    "print(f\"BLEU-2 Score: {bleu2:.2f}\")\n",
    "print(f\"BLEU-4 Score: {bleu4:.2f}\")\n",
    "print(f\"ROUGE-L Score: {rouge_l:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2. CLIP-Based Evaluation: Top-K Retrieval ----\n",
    "\n",
    "def precision_recall_at_k(gt_list, pred_list, k):\n",
    "    precision_list, recall_list = [], []\n",
    "    for gt, pred in zip(gt_list, pred_list):\n",
    "        pred_k = pred[:k]\n",
    "        hits = len(set(gt).intersection(pred_k))\n",
    "        precision = hits / k\n",
    "        recall = hits / len(gt) if len(gt) > 0 else 0\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "    return np.mean(precision_list), np.mean(recall_list)\n",
    "\n",
    "# Sample input (replace with your own indices)\n",
    "ground_truth_indices = [[101], [202], [303]]\n",
    "predicted_indices = [[101, 102, 103], [204, 202, 206], [307, 303, 301]]\n",
    "\n",
    "for k in [1, 3, 5]:\n",
    "    p_at_k, r_at_k = precision_recall_at_k(ground_truth_indices, predicted_indices, k)\n",
    "    print(f\"Precision@{k}: {p_at_k:.2f}, Recall@{k}: {r_at_k:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
